{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinearClassifier(BaseEstimator):\n",
    "      \"\"\"\n",
    "      General class for binary linear classifiers. Implements the predict\n",
    "      function, which is the same for all binary linear classifiers. There are\n",
    "      also two utility functions.\n",
    "      \"\"\"\n",
    "\n",
    "      def decision_function(self, X):\n",
    "            \"\"\"\n",
    "            Computes the decision function for the inputs X. The inputs are assumed to be\n",
    "            stored in a matrix, where each row contains the features for one\n",
    "            instance.\n",
    "            \"\"\"\n",
    "            return X.dot(self.w)\n",
    "\n",
    "      def predict(self, X):\n",
    "            \"\"\"\n",
    "            Predicts the outputs for the inputs X. The inputs are assumed to be\n",
    "            stored in a matrix, where each row contains the features for one\n",
    "            instance.\n",
    "            \"\"\"\n",
    "\n",
    "            # First compute the output scores\n",
    "            scores = self.decision_function(X)\n",
    "\n",
    "            # Select the positive or negative class label, depending on whether\n",
    "            # the score was positive or negative.\n",
    "            out = np.select([scores >= 0.0, scores < 0.0],\n",
    "                            [self.positive_class,\n",
    "                             self.negative_class])\n",
    "            return out\n",
    "\n",
    "      def find_classes(self, Y):\n",
    "            \"\"\"\n",
    "            Finds the set of output classes in the output part Y of the training set.\n",
    "            If there are exactly two classes, one of them is associated to positive\n",
    "            classifier scores, the other one to negative scores. If the number of\n",
    "            classes is not 2, an error is raised.\n",
    "            \"\"\"\n",
    "            classes = sorted(set(Y))\n",
    "            if len(classes) != 2:\n",
    "                  raise Exception(\"this does not seem to be a 2-class problem\")\n",
    "            self.positive_class = classes[1]\n",
    "            self.negative_class = classes[0]\n",
    "\n",
    "      def encode_outputs(self, Y):\n",
    "            \"\"\"\n",
    "            A helper function that converts all outputs to +1 or -1.\n",
    "            \"\"\"\n",
    "            return np.array([1 if y == self.positive_class else -1 for y in Y])\n",
    "\n",
    "\n",
    "class Perceptron(LinearClassifier):\n",
    "      \"\"\"\n",
    "      A straightforward implementation of the perceptron learning algorithm.\n",
    "      \"\"\"\n",
    "\n",
    "      def __init__(self, n_iter=20):\n",
    "            \"\"\"\n",
    "            The constructor can optionally take a parameter n_iter specifying how\n",
    "            many times we want to iterate through the training set.\n",
    "            \"\"\"\n",
    "            self.n_iter = n_iter\n",
    "\n",
    "      def fit(self, X, Y):\n",
    "            \"\"\"\n",
    "            Train a linear classifier using the perceptron learning algorithm.\n",
    "            \"\"\"\n",
    "\n",
    "            # First determine which output class will be associated with positive\n",
    "            # and negative scores, respectively.\n",
    "            self.find_classes(Y)\n",
    "\n",
    "            # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "            Ye = self.encode_outputs(Y)\n",
    "\n",
    "            # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "            # into a normal NumPy matrix.\n",
    "            if not isinstance(X, np.ndarray):\n",
    "                  X = X.toarray()\n",
    "\n",
    "            # Initialize the weight vector to all zeros.\n",
    "            n_features = X.shape[1]\n",
    "            self.w = np.zeros(n_features)\n",
    "\n",
    "            # Perceptron algorithm:\n",
    "            for i in range(self.n_iter):\n",
    "                  for x, y in zip(X, Ye):\n",
    "\n",
    "                        # Compute the output score for this instance.\n",
    "                        score = x.dot(self.w)\n",
    "\n",
    "                        # If there was an error, update the weights.\n",
    "                        if y * score <= 0:\n",
    "                              self.w += y * x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.00 sec.\n",
      "Accuracy: 0.7919.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# This function reads the corpus, returns a list of documents, and a list\n",
    "# of their corresponding polarity labels.\n",
    "def read_data(corpus_file):\n",
    "      X = []\n",
    "      Y = []\n",
    "      with open(corpus_file, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                  _, y, _, x = line.split(maxsplit=3)\n",
    "                  X.append(x.strip())\n",
    "                  Y.append(y)\n",
    "      return X, Y\n",
    "\n",
    "\n",
    "# Read all the documents.\n",
    "X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "# Split into training and test parts.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                random_state=0)\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "      TfidfVectorizer(),\n",
    "      SelectKBest(k=1000),\n",
    "      Normalizer(),\n",
    "      # NB that this is our Perceptron, not sklearn.linear_model.Perceptron\n",
    "      Perceptron()\n",
    ")\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "class PegasosSVC(LinearClassifier):\n",
    "      \"\"\"\n",
    "      Pegasos Support Vector Classifier\n",
    "      \"\"\"\n",
    "\n",
    "      def __init__(self, n_iter=20, lam=1):\n",
    "            \"\"\"\n",
    "            The constructor can optionally take a parameter n_iter specifying how\n",
    "            many times we want to iterate through the training set.\n",
    "            \"\"\"\n",
    "            self.n_iter = n_iter\n",
    "            self.lam = lam\n",
    "\n",
    "      def fit(self, X, Y):\n",
    "            \"\"\"\n",
    "            Train a linear classifier using the perceptron learning algorithm.\n",
    "            \"\"\"\n",
    "\n",
    "            self.find_classes(Y)\n",
    "\n",
    "            Ye = self.encode_outputs(Y)\n",
    "\n",
    "            if not isinstance(X, np.ndarray):\n",
    "                  X = X.toarray()\n",
    "\n",
    "            # Initialize the weight vector to all zeros.\n",
    "            n_features = X.shape[1]\n",
    "            m = X.shape[0]\n",
    "            self.w = np.zeros(n_features)\n",
    "\n",
    "            for i in range(1, self.n_iter + 1):\n",
    "\n",
    "                  j = np.random.choice(m, 1)[0]\n",
    "                  x, y = X[j], Ye[j]\n",
    "                  nu = 1. / (self.lam * i)\n",
    "                  score = self.w.dot(x)\n",
    "                  if y * score < 1:\n",
    "                        self.w = (1 - nu * self.lam) * self.w + (nu * y) * x\n",
    "                  else:\n",
    "                        self.w = (1 - nu * self.lam) * self.w\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.78 sec.\n",
      "Accuracy: 0.8095.\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "      TfidfVectorizer(),\n",
    "      SelectKBest(k=1000),\n",
    "      Normalizer(),\n",
    "      PegasosSVC(lam=1/len(Xtrain), n_iter=10000)\n",
    ")\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "class PegasosLR(LinearClassifier):\n",
    "      \"\"\"\n",
    "      Pegasos Logistic Regression\n",
    "      \"\"\"\n",
    "\n",
    "      def __init__(self, n_iter=20, lam=1):\n",
    "            \"\"\"\n",
    "            The constructor can optionally take a parameter n_iter specifying how\n",
    "            many times we want to iterate through the training set.\n",
    "            \"\"\"\n",
    "            self.n_iter = n_iter\n",
    "            self.lam = lam\n",
    "\n",
    "      def fit(self, X, Y):\n",
    "            \"\"\"\n",
    "            Train a linear classifier using the perceptron learning algorithm.\n",
    "            \"\"\"\n",
    "\n",
    "            self.find_classes(Y)\n",
    "\n",
    "            Ye = self.encode_outputs(Y)\n",
    "\n",
    "            if not isinstance(X, np.ndarray):\n",
    "                  X = X.toarray()\n",
    "\n",
    "            # Initialize the weight vector to all zeros.\n",
    "            n_features = X.shape[1]\n",
    "            m = X.shape[0]\n",
    "            self.w = np.zeros(n_features)\n",
    "\n",
    "            for i in range(1, self.n_iter + 1):\n",
    "                  j = np.random.choice(m, 1)[0]\n",
    "                  x, y = X[j], Ye[j]\n",
    "                  nu = 1. / (self.lam * i)\n",
    "                  score = self.w.dot(x)\n",
    "                  loss_gradient = -(y / (1 + np.exp(y * score))) * x\n",
    "                  self.w = (1 - nu * self.lam) * self.w - nu * loss_gradient\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.80 sec.\n",
      "Accuracy: 0.8208.\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "      TfidfVectorizer(),\n",
    "      SelectKBest(k=1000),\n",
    "      Normalizer(),\n",
    "      PegasosLR(lam=1/len(Xtrain), n_iter=10000)\n",
    ")\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "from scipy.linalg.blas import ddot, dscal, daxpy\n",
    "\n",
    "class PegasosSVCFast(LinearClassifier):\n",
    "      \"\"\"\n",
    "      Pegasos Support Vector Classifier with performance improvements\n",
    "      \"\"\"\n",
    "\n",
    "      def __init__(self, n_iter=20, lam=1):\n",
    "            \"\"\"\n",
    "            The constructor can optionally take a parameter n_iter specifying how\n",
    "            many times we want to iterate through the training set.\n",
    "            \"\"\"\n",
    "            self.n_iter = n_iter\n",
    "            self.lam = lam\n",
    "\n",
    "      def fit(self, X, Y):\n",
    "            \"\"\"\n",
    "            Train a linear classifier using the perceptron learning algorithm.\n",
    "            \"\"\"\n",
    "\n",
    "            self.find_classes(Y)\n",
    "\n",
    "            Ye = self.encode_outputs(Y)\n",
    "\n",
    "            if not isinstance(X, np.ndarray):\n",
    "                  X = X.toarray()\n",
    "\n",
    "            # Initialize the weight vector to all zeros.\n",
    "            n_features = X.shape[1]\n",
    "            m = X.shape[0]\n",
    "            self.w = np.zeros(n_features)\n",
    "\n",
    "            for i in range(1, self.n_iter + 1):\n",
    "\n",
    "                  j = np.random.choice(m, 1)[0]\n",
    "                  x, y = X[j], Ye[j]\n",
    "                  nu = 1. / (self.lam * i)\n",
    "                  #score = self.w.dot(x)\n",
    "                  score = ddot(self.w, x)\n",
    "                  if y * score < 1:\n",
    "                        #self.w = (1 - nu * self.lam) * self.w + (nu * y) * x\n",
    "                        #self.w = dscal((1 - nu * self.lam), self.w) + dscal(nu * y, x)\n",
    "                        dscal((1 - nu * self.lam), self.w)\n",
    "                        #y += a*x == y + a*x\n",
    "                        daxpy(y=self.w, x=x, a=nu*y)\n",
    "                  else:\n",
    "                        #x *= 3 == x = x * 3\n",
    "                        #self.w *= (1 - nu * self.lam)\n",
    "                        #self.w = (1 - nu * self.lam) * self.w\n",
    "                        #self.w = dscal((1 - nu * self.lam), self.w)\n",
    "                        dscal((1 - nu * self.lam), self.w)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 2.04 sec.\n",
      "Accuracy: 0.8389.\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "      TfidfVectorizer(),\n",
    "      SelectKBest(k=1000),\n",
    "      Normalizer(),\n",
    "      PegasosSVCFast(lam=1/len(Xtrain), n_iter=100000)\n",
    ")\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 2.06 sec.\n",
      "Accuracy: 0.8321.\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "      TfidfVectorizer(),\n",
    "      SelectKBest(k=1000),\n",
    "      Normalizer(),\n",
    "      PegasosSVC(lam=1/len(Xtrain), n_iter=100000)\n",
    ")\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "def add_sparse_to_dense(x, w, factor):\n",
    "      \"\"\"\n",
    "      Adds a sparse vector x, scaled by some factor, to a dense vector.\n",
    "      This can be seen as the equivalent of w += factor * x when x is a dense\n",
    "      vector.\n",
    "      \"\"\"\n",
    "      w[x.indices] += factor * x.data\n",
    "\n",
    "\n",
    "def sparse_dense_dot(x, w):\n",
    "      \"\"\"\n",
    "      Computes the dot product between a sparse vector x and a dense vector w.\n",
    "      \"\"\"\n",
    "      return np.dot(w[x.indices], x.data)\n",
    "\n",
    "class PegasosSVCSparse(LinearClassifier):\n",
    "      \"\"\"\n",
    "      Pegasos Support Vector classifier using sparse matrices\n",
    "      \"\"\"\n",
    "\n",
    "      def __init__(self, n_iter=20, lam=1):\n",
    "            \"\"\"\n",
    "            The constructor can optionally take a parameter n_iter specifying how\n",
    "            many times we want to iterate through the training set.\n",
    "            \"\"\"\n",
    "            self.n_iter = n_iter\n",
    "            self.lam = lam\n",
    "\n",
    "      def fit(self, X, Y):\n",
    "            \"\"\"\n",
    "            Train a linear classifier using the perceptron learning algorithm.\n",
    "            \"\"\"\n",
    "\n",
    "            self.find_classes(Y)\n",
    "\n",
    "            Ye = self.encode_outputs(Y)\n",
    "\n",
    "\n",
    "            # Initialize the weight vector to all zeros.\n",
    "            n_features = X.shape[1]\n",
    "            m = X.shape[0]\n",
    "            self.w = np.zeros(n_features)\n",
    "            for i in range(1, self.n_iter + 1):\n",
    "\n",
    "                  j = np.random.choice(m, 1)[0]\n",
    "                  x, y = X[j], Ye[j]\n",
    "                  nu = 1. / (self.lam * i)\n",
    "                  score = sparse_dense_dot(x=x, w=self.w)\n",
    "                  self.w = (1 - nu * self.lam) * self.w\n",
    "                  if y * score < 1:\n",
    "                        add_sparse_to_dense(x = x, w = self.w, factor = (nu * y))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.38 sec.\n",
      "Accuracy: 0.8141.\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "      TfidfVectorizer(),\n",
    "      Normalizer(),\n",
    "      PegasosSVCSparse(lam=1/len(Xtrain), n_iter=10000)\n",
    ")\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "class PegasosSVCSparseFastScaling(LinearClassifier):\n",
    "      \"\"\"\n",
    "      Pegasos Support Vector Classifier using sparse matrices and performance improvements\n",
    "      \"\"\"\n",
    "\n",
    "      def __init__(self, n_iter=20, lam=1):\n",
    "            \"\"\"\n",
    "            The constructor can optionally take a parameter n_iter specifying how\n",
    "            many times we want to iterate through the training set.\n",
    "            \"\"\"\n",
    "            self.n_iter = n_iter\n",
    "            self.lam = lam\n",
    "\n",
    "      def fit(self, X, Y):\n",
    "            \"\"\"\n",
    "            Train a linear classifier using the perceptron learning algorithm.\n",
    "            \"\"\"\n",
    "\n",
    "            self.find_classes(Y)\n",
    "\n",
    "            Ye = self.encode_outputs(Y)\n",
    "\n",
    "\n",
    "            # Initialize the weight vector to all zeros.\n",
    "            n_features = X.shape[1]\n",
    "            m = X.shape[0]\n",
    "            self.w = np.zeros(n_features)\n",
    "\n",
    "\n",
    "            a = 1\n",
    "            for i in range(2, self.n_iter + 2):\n",
    "\n",
    "                  j = np.random.choice(m, 1)[0]\n",
    "                  x, y = X[j], Ye[j]\n",
    "                  nu = 1. / (self.lam * i)\n",
    "                  score = a * sparse_dense_dot(x=x, w=self.w)\n",
    "\n",
    "                  if y * score < 1:\n",
    "                        add_sparse_to_dense(x = x, w = self.w, factor = (nu * y) /a)\n",
    "                  a = (1 - nu * self.lam) * a\n",
    "            self.w = a * self.w"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.18 sec.\n",
      "Accuracy: 0.8120.\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "      TfidfVectorizer(),\n",
    "      Normalizer(),\n",
    "      PegasosSVCSparseFastScaling(lam=1/len(Xtrain), n_iter=10000)\n",
    ")\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
